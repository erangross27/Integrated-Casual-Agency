# Continuous Learning Configuration
# Optimized settings for long-running continuous learning sessions

causal_graph:
  initial_nodes: 100                    # Start smaller for faster iteration
  initial_edges: 150
  confidence_threshold: 0.15            # Slightly higher threshold for quality
  max_nodes: 100000                     # Allow larger graphs for long sessions
  pruning_interval: 1000                # Prune low-confidence edges periodically
  min_confidence_for_persistence: 0.3   # Higher bar for permanent knowledge

world_model:
  embedding_dim: 128                    # Balanced size for efficiency
  hidden_dim: 256
  num_layers: 3
  dropout: 0.1
  learning_rate: 0.001
  weight_decay: 1e-5
  batch_size: 64                        # Smaller batches for continuous learning
  
  # Uncertainty quantification
  epistemic_samples: 10                 # Reduced for speed
  aleatoric_head_dim: 64
  uncertainty_weight: 0.1
  
  # State dimensions
  state_dim: 20                         # Manageable state space
  action_dim: 8
  num_relations: 25

curiosity:
  prediction_weight: 1.0
  entropy_weight: 0.1
  novelty_weight: 0.05
  eta: 0.01                            # Learning rate for curiosity
  complexity_penalty: 0.01              # Encourage simpler explanations
  exploration_bonus_decay: 0.999       # Gradually reduce exploration
  min_exploration: 0.1                  # Maintain minimum exploration

action_planning:
  algorithm: "sac"                      # Soft Actor-Critic
  actor_lr: 0.0003
  critic_lr: 0.0003
  alpha_lr: 0.0003                     # Entropy coefficient learning rate
  buffer_size: 50000                   # Reduced buffer for memory efficiency
  batch_size: 128
  gamma: 0.99
  tau: 0.005
  target_update_interval: 1
  
  # Experience replay
  her_ratio: 0.8                       # Hindsight Experience Replay
  prioritized_replay: true
  priority_alpha: 0.6
  priority_beta: 0.4

abstraction:
  motif_min_size: 2
  motif_max_size: 4                    # Smaller motifs for efficiency
  clustering_algorithm: "kmeans"
  num_clusters: 8                      # Reduced for faster clustering
  max_motifs_per_iteration: 100        # Limit motif discovery
  utility_decay: 0.99                  # Faster utility decay
  abstraction_interval: 500            # Run abstraction every N steps
  concept_coherence_threshold: 0.6     # Quality threshold for concepts

# Continuous learning specific settings
continuous_learning:
  environment_complexity_start: 1.0
  complexity_growth_rate: 0.0005       # Slower growth for stability
  entity_pool_size: 50                 # Initial entity pool
  relation_pool_size: 30               # Initial relation pool
  new_entity_interval: 2000            # Add new entities every N steps
  new_relation_interval: 3000          # Add new relations every N steps
  
  # Performance optimization
  memory_cleanup_interval: 5000        # Clean up old data every N steps
  checkpoint_interval: 10000           # Save checkpoints every N steps
  max_history_length: 50000            # Limit history to prevent memory issues
  
  # Adaptive learning
  adaptive_learning_rate: true         # Adjust learning based on performance
  performance_window: 1000             # Window for performance evaluation
  learning_rate_min: 0.0001
  learning_rate_max: 0.001
  
  # Exploration schedule
  exploration_schedule: "exponential"  # exponential, linear, or constant
  initial_exploration: 1.0
  final_exploration: 0.1
  exploration_decay_steps: 100000

# Logging and monitoring
logging:
  level: "INFO"
  log_file: "logs/continuous_learning.log"
  max_log_size: "100MB"
  backup_count: 5
  
  # Performance logging
  log_performance_interval: 100        # Log performance every N steps
  log_detailed_metrics: false          # Detailed metrics (slower)
  
monitoring:
  save_interval_minutes: 10            # Save progress every N minutes
  metrics_history_length: 10000        # Keep last N metrics records
  enable_real_time_monitoring: true    # Enable real-time dashboard
  
  # Visualization settings
  plot_update_interval: 30             # Update plots every N seconds
  max_plot_points: 1000               # Limit plot points for performance

# Hardware optimization
hardware:
  use_cuda: true                       # Use GPU if available
  cuda_device: 0                       # CUDA device ID
  num_workers: 2                       # Number of worker threads
  pin_memory: true                     # Pin memory for faster GPU transfer
  
  # Memory management
  memory_limit_gb: 8                   # Soft memory limit
  garbage_collection_interval: 1000    # Force GC every N steps
  
# Safety and robustness
safety:
  max_prediction_error: 10.0          # Clamp extreme prediction errors
  confidence_bounds: [0.001, 0.999]   # Prevent extreme confidence values
  gradient_clipping: 1.0              # Clip gradients to prevent instability
  nan_handling: "zero"                # How to handle NaN values
  
  # Emergency stops
  enable_emergency_stops: true        # Enable automatic emergency stops
  max_consecutive_errors: 10          # Stop after N consecutive errors
  min_confidence_threshold: 0.01      # Stop if confidence drops too low
  
# Experimental features
experimental:
  enable_meta_learning: false         # Meta-learning across sessions
  enable_transfer_learning: false     # Transfer between different environments
  enable_distributed_learning: false  # Distributed learning (future)
  
  # Advanced techniques
  enable_curriculum_learning: true    # Gradually increase difficulty
  enable_self_play: false            # Self-play for multi-agent scenarios
  enable_hierarchical_rl: false      # Hierarchical reinforcement learning
